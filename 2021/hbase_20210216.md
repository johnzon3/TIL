# TIL 20210216
## 오늘 공부한 내용  

# hbase hfile에 적재되는 Cell의 Type에 대한 설명  

https://stackoverflow.com/questions/46760562/how-is-the-type-of-a-cell-used-in-hbase  
http://hadoop-hbase.blogspot.com/2012/01/scanning-in-hbase.html  
  
https://hbase.apache.org/devapidocs/src-html/org/apache/hadoop/hbase/Cell.Type.html#line.227  

```
Cell.Type
Put((byte) 4),
Delete((byte) 8),
DeleteFamilyVersion((byte) 10),
DeleteColumn((byte) 12),
DeleteFamily((byte) 14);
```

hfile은 immutable한데, 한번 작성되면 변하지 않고 각 row(key)값에 변한 내용을 hfile에 추가하여 작성한다.(compaction하기 전까지)  
그에 대한 내용에 대한 조사를 하다가 Cell.Type에 delete, DeleteFamilyVersion등등의 type이 존재하는데  
어떤 operation을 수행했을때 각각의 타입이 나오는지 궁금했다.  

hbase를 standalone으로 기동하고 각 hfile을 spark로 읽는 것을 목표로 한다.  

# 사전 준비

테스트에 사용된 파일들 :  
http://archive.apache.org/dist/hbase/1.4.10/  
spark 2.4.4_2.11.12  
spark에는 hbase를 읽어들이기 위해 다음의 라이브러리를 추가하였다.(htrace는 없어서 따로 받음)

```
[root@b3c86bb217b7 jars]# ls /opt/spark/jars/hbase*
/opt/spark/jars/hbase-client-2.0.0-alpha4.jar                /opt/spark/jars/hbase-mapreduce-2.0.0-alpha4.jar        /opt/spark/jars/hbase-replication-2.0.0-alpha4.jar
/opt/spark/jars/hbase-common-2.0.0-alpha4-tests.jar          /opt/spark/jars/hbase-metrics-2.0.0-alpha4.jar          /opt/spark/jars/hbase-server-2.0.0-alpha4.jar
/opt/spark/jars/hbase-common-2.0.0-alpha4.jar                /opt/spark/jars/hbase-metrics-api-2.0.0-alpha4.jar      /opt/spark/jars/hbase-shaded-miscellaneous-1.0.1.jar
/opt/spark/jars/hbase-hadoop-compat-2.0.0-alpha4.jar         /opt/spark/jars/hbase-prefix-tree-2.0.0-alpha4.jar      /opt/spark/jars/hbase-shaded-netty-1.0.1.jar
/opt/spark/jars/hbase-hadoop2-compat-2.0.0-alpha4-tests.jar  /opt/spark/jars/hbase-procedure-2.0.0-alpha4.jar        /opt/spark/jars/hbase-shaded-protobuf-1.0.1.jar
/opt/spark/jars/hbase-hadoop2-compat-2.0.0-alpha4.jar        /opt/spark/jars/hbase-protocol-2.0.0-alpha4.jar
/opt/spark/jars/hbase-http-2.0.0-alpha4.jar                  /opt/spark/jars/hbase-protocol-shaded-2.0.0-alpha4.jar
[root@b3c86bb217b7 jars]# wget https://repo1.maven.org/maven2/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar
```

그리고 spark-shell로는 다음의 script만 실행시킨다.  
계속 test()라는 함수를 호출하면서 hfile을 읽을것이다.  

```
import org.apache.hadoop.hbase.mapreduce.HFileInputFormat
import org.apache.hadoop.io.NullWritable
import org.apache.hadoop.hbase.Cell
import org.apache.hadoop.hbase.CellUtil
import org.apache.hadoop.hbase.util.Bytes

def test(){
    // default local standalone hbase root path.
    val path = "file:///tmp/hbase-root/hbase/data/default/test/*/*/*"
    val input = sc.newAPIHadoopFile(path, classOf[HFileInputFormat], classOf[NullWritable],classOf[Cell])

    case class Record(row: String, family: String, qualifier: String, timestamp: Long, code: String, value: String)

    val recordrdd = input.map(
        record => (
            Record(
                new String(CellUtil.cloneRow(record._2)),
                new String(CellUtil.cloneFamily(record._2)),
                new String(CellUtil.cloneQualifier(record._2)),
                record._2.getTimestamp,
                record._2.getTypeByte.toString,
                new String(CellUtil.cloneValue(record._2))
            )
        )
    )
    recordrdd.take(100).foreach(println)
}
```

# 실험 시작

## 1. 데이터 입력 후 
hbase shell로는 다음의 동작을 수행해본다.

```
hbase(main):007:0> create 'test', 'cf'
0 row(s) in 1.2570 seconds

=> Hbase::Table - test
hbase(main):008:0> put 'test', 'row1', 'cf:a', 'value1'
0 row(s) in 0.0930 seconds

hbase(main):009:0> put 'test', 'row2', 'cf:b', 'value2'
0 row(s) in 0.0040 seconds

hbase(main):010:0> put 'test', 'row3', 'cf:c', 'value3'
0 row(s) in 0.0070 seconds
```

```
scala> test()
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input Pattern file:/tmp/hbase-root/hbase/data/default/test/*/*/* matches 0 files
  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:329)
  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:271)
  at org.apache.hadoop.hbase.mapreduce.HFileInputFormat.listStatus(HFileInputFormat.java:150)
  at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:393)
  at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
  at org.apache.spark.rdd.RDD.take(RDD.scala:1337)
  at test(<console>:55)
  ... 49 elided
```

그러나 아무것도 보이지 않는 것을 확인하고 flush command를 깜빡한것을 깨달았다.

```
hbase(main):011:0> flush 'test'
0 row(s) in 0.1870 seconds
```

```
scala> test()
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
Record(row3,cf,c,1613462460960,4,value3)
```

## 2. 데이터 변경과 추가 후 확인

테스트를 위해 row3에 column a를 추가하고 column c의 value를 value33으로 변경해본다

```
hbase(main):012:0> put 'test', 'row3', 'cf:a', 'value31'
0 row(s) in 0.0170 seconds

hbase(main):013:0> put 'test', 'row3', 'cf:c', 'value33'
0 row(s) in 0.0060 seconds

hbase(main):014:0> flush 'test'
0 row(s) in 0.1640 seconds
```

spark에서 확인해보면 다음과 같다
```
scala> test()
Record(row3,cf,a,1613462688435,4,value31)
Record(row3,cf,c,1613462688468,4,value33)
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
Record(row3,cf,c,1613462460960,4,value3)
```

## 3. delete operation수행 이후 

역시 hfile에 쓰는 행위는 이전에 row3 value3도 기록되어있는걸 확인할수 있다.  
이때 delete operation을 수행하면 어떻게 될까?  

```
hbase(main):015:0> delete 'test', 'row3', 'cf:c'
0 row(s) in 0.0260 seconds

hbase(main):016:0> scan 'test'
ROW                                              COLUMN+CELL
 row1                                            column=cf:a, timestamp=1613462459943, value=value1
 row2                                            column=cf:b, timestamp=1613462459998, value=value2
 row3                                            column=cf:a, timestamp=1613462688435, value=value31
 row3                                            column=cf:c, timestamp=1613462460960, value=value3
3 row(s) in 0.0250 seconds
```

아직 flush는 수행하지 않았고, scan으로 hbase shell에서 확인해보았다.  
cf:c의 value를 value3에서 value33으로 변경했고, 그 이후에 cf:c의 값을 지우니, 이전에 세팅되어있던 value3으로 변경되어버렸다.  

flush를 수행해주고 spark에서 hfile을 확인해보면..  

```
hbase(main):017:0> flush 'test'
0 row(s) in 0.1620 seconds
#############################spark-shell

scala> test()
Record(row3,cf,a,1613462688435,4,value31)
Record(row3,cf,c,1613462688468,4,value33)
Record(row3,cf,c,1613462688468,8,)
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
Record(row3,cf,c,1613462460960,4,value3)
```

`Record(row3,cf,c,1613462688468,8,)` 8번 타입이 생긴것을 볼수 있는데, 이것은...

```
Cell.Type
Put((byte) 4),
Delete((byte) 8),
DeleteFamilyVersion((byte) 10),
DeleteColumn((byte) 12),
DeleteFamily((byte) 14);
```

delete operation을 수행한것도 볼수 있다.  
나는 원하는게 DeleteFamily는 어떤 operation을 수행해야 보이나가 궁금한건데..

## 4. deleteall operation확인

다음의 operation을 수행해보았다.

```
hbase(main):018:0> deleteall 'test', 'row3'
0 row(s) in 0.0130 seconds

hbase(main):019:0> scan 'test'
ROW                                              COLUMN+CELL
 row1                                            column=cf:a, timestamp=1613462459943, value=value1
 row2                                            column=cf:b, timestamp=1613462459998, value=value2
2 row(s) in 0.0110 seconds

hbase(main):020:0> flush 'test'
0 row(s) in 0.1560 seconds
```

```
scala> test()
Record(row3,cf,,1613463175225,14,)
Record(row3,cf,a,1613462688435,4,value31)
Record(row3,cf,c,1613462688468,4,value33)
Record(row3,cf,c,1613462688468,8,)
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
Record(row3,cf,c,1613462460960,4,value3)
```
deleteall로 인해서 row3컬럼은 아예 삭제된 것을 확인했고, hfile에는 14 Type이 새로 생긴것을 확인하였다.


## 5. compaction확인

근데 이런식으로 계속 hfile에 append되면 그것을 하나로 정리할 방법이 없을까가 compaction이다.
일단 major_compaction으로 실행해본다.

```
hbase(main):022:0> major_compact 'test'
0 row(s) in 0.0390 seconds
```

```
scala> test()
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
Record(row3,cf,,1613463175225,14,)
Record(row3,cf,a,1613462688435,4,value31)
Record(row3,cf,c,1613462688468,4,value33)
Record(row3,cf,c,1613462688468,8,)
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
Record(row3,cf,c,1613462460960,4,value3)
```

https://stackoverflow.com/questions/63650713/hbasedifference-between-minor-and-major-compaction  
minor, major_compact에 대한 글
하나의 파일로 통합한다해서 이전의 이력을 다 지워주는줄 알았더니 그게 아니였다.

실제로는 compact명령어를 실행하게 되면 원하는 동작을 확인할수 있다.

```
hbase(main):023:0> compact 'test'
0 row(s) in 0.0300 seconds
```

```
scala> test()
Record(row1,cf,a,1613462459943,4,value1)
Record(row2,cf,b,1613462459998,4,value2)
```

# 오늘의 결론

Hbase에서는 Hfile에 각각의 row를 Cell형태로 정리하게 되는데  
각 Cell이 어떤 내용으로 적히는지, compaction을 하게 되면 Hfile내부적으로는 어떻게 바뀌는지 확인하였다.

